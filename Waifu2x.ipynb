{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Waifu2x.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stalgiag/Waifu2x/blob/master/Waifu2x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VFvWTMpQsd_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check for CUDA"
      ]
    },
    {
      "metadata": {
        "id": "cEmh8QZar2hp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOWhwi4rtDS8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Install Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "engciW6zshoc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# different torch install needed\n",
        "# !pip3 uninstall torch\n",
        "!pip3 install -U https://download.pytorch.org/whl/cu100/torch-1.0.0-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NEduSAUYtJMd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check Pytorch Install Works with Cuda"
      ]
    },
    {
      "metadata": {
        "id": "IvoIKI7ktNPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XLx_9yLJun-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we will verify that GPU is enabled for this notebook\n",
        "# following should print: CUDA is available!  Training on GPU ...\n",
        "# \n",
        "# if it prints otherwise, then you need to enable GPU: \n",
        "# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lz-NtXA7uqTc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check that gcc is installed\n",
        "# should print version number along bottom\n",
        "!gcc -v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KONlz1gb8g7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get gpu type\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EsEBnGgHXc9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clone my modified fork"
      ]
    },
    {
      "metadata": {
        "id": "JubU-aj7vdd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#clone repo - my fork has a few simple mods that make this work with colab\n",
        "!git clone https://github.com/stalgiag/Waifu2x.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3IqSiw6RHbp8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setup"
      ]
    },
    {
      "metadata": {
        "id": "U-lYOWzX84Fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd Waifu2x\n",
        "unzip model_check_points/CRAN_V2/CRAN_V2_02_28_2019.zip -d model_check_points/CRAN_V2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xOGNjS3x-Tei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd ~/../content/\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0NJEIYVABjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06bd4cc0-3b92-4f8d-9f8c-fba2ff8b7f29"
      },
      "cell_type": "code",
      "source": [
        "%cd ~/../content/Waifu2x"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Waifu2x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dX7Zjffj_dDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##test to see what kind of changes we have\n",
        "from utils.prepare_images import *\n",
        "from Models import *\n",
        "from torchvision.utils import save_image\n",
        "model_cran_v2 = CARN_V2(color_channels=3, mid_channels=64, conv=nn.Conv2d,\n",
        "                        single_conv_size=3, single_conv_group=1,\n",
        "                        scale=2, activation=nn.LeakyReLU(0.1),\n",
        "                        SEBlock=True, repeat_blocks=3, atrous=(1, 1, 1))\n",
        "                        \n",
        "model_cran_v2 = network_to_half(model_cran_v2)\n",
        "checkpoint = \"model_check_points/CRAN_V2/CARN_model_checkpoint.pt\"\n",
        "model_cran_v2.load_state_dict(torch.load(checkpoint, 'cpu'))\n",
        "# if use GPU, then comment out the next line so it can use fp16. \n",
        "model_cran_v2 = model_cran_v2.float() \n",
        "\n",
        "demo_img = \"out.png\"\n",
        "img = load_single_image(demo_img,\n",
        "                      up_scale=True,\n",
        "                      up_scale_factor=2,\n",
        "                      up_scale_method=Image.BILINEAR)\n",
        "img_t = img[1]\n",
        "\n",
        "img_b = Image.open(demo_img).convert(\"RGB\")\n",
        "\n",
        "img_splitter = ImageSplitter(seg_size=64, scale_factor=2, boarder_pad_size=3)\n",
        "img_patches = img_splitter.split_img_tensor(img_b, scale_method=None, img_pad=0)\n",
        "with torch.no_grad():\n",
        "    out = [model_cran_v2(i) for i in img_patches]\n",
        "img_upscale = img_splitter.merge_img_tensor(out)\n",
        "\n",
        "final = torch.cat([img_t, img_upscale])\n",
        "save_image(img_upscale, 'out.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}